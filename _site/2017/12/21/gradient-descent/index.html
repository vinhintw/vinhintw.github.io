<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Gradient Descent &middot; Vinh Ng
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/lanyon.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-precomposed.png">
  <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">
  <!-- <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> -->

  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-BF3TPWNBEM"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-BF3TPWNBEM');
  </script>

  <!-- toogle button -->
  <script>
  function myFunction() {
      var x = document.getElementById("myDIV");
      if (x.style.display === "none") {
          x.style.display = "block";
      } else {
          x.style.display = "none";
      }
  }
  </script>
  
</head>


  <body>

    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <p>A personal website of <a href="https://vinhintw.github.io/about" target="">Vinh Ng</a>, containing technical blog on technology.</p>
  </div>

  <nav class="sidebar-nav">
    <a class="sidebar-nav-item" href="/">Home</a>

    

    
    
      
        
      
    
      
        
      
    
      
        
          <a class="sidebar-nav-item" href="/about/">About</a>
        
      
    
      
    
      
        
      
    
      
        
      
    
    <span class="sidebar-nav-item">Currently v1.0.0</span>
  </nav>

  <div class="sidebar-item">
    <p>
      &copy; 2023. All rights reserved.
    </p>
  </div>
</div>

 
    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap">
      <div class="masthead">
        <div class="container">
          <h3 class="masthead-title">
            <a href="/" title="Home">Vinh Ng</a>
            <small>blog</small>
          </h3>
        </div>
      </div>

      <link rel="stylesheet" href="/public/css/blog-index.css">

<nav>
	<div class='blog-wrapper'>
		<div class='blog-index'>
			<strong><u>Machine learning</u></strong></br>
				<li><a href=''>6. Gradient Descent</a></li>
				<li><a href=''>5. K Nearest Neighbors</a></li>
				<li><a href=''>4. Linear Regression</a></li>
				<li><a href=''>3. AI for flappy bird game (Neural Network, Genetic Algorithm)</a></li>
				<li><a href=''>2. Image Compression using K-Means</a></li>
				<li><a href=''>1. K-Means Clustering</a></li>
			<strong><u>Data Structure and Algorithm</u></strong></br>
				<li><a href=''>1. Linked List</a></li>
				<li><a href=''>2. Stack, Queue</a>
			<strong><u>Other</u></strong></br>
				<li><a href=''>4. Mapping MySQL to MongoDB</a></li>
				<li><a href=''>3. University MySQL DB</a></li>
				<li><a href=''>2. Planetary Rover Game (C#, Winform)</a></li>
				<li><a href=''>1. Music Player (Python)</a></li>
			<strong><u>Project</u></strong></br>
		</div>
	</div>
</nav>

      <div class="container content">
        <div class="post">
  <h1 style="font-size: 130%;" class="post-title">Gradient Descent</h1>
  <span class="post-date" style="float:none;">21 Dec 2017</span>
  
  <div class="tag">Calculus</div>
<div class="tag">Regression</div>
<div class="tag">Optimization</div>

<p>An optimization algorithm which uses gradient value of cost function to recursively adjust the solution of optimization problem.</p>

<p><strong>Content:</strong>
<!-- MarkdownTOC depth=3 --></p>

<ul>
  <li><a href="#1-introduction">1. Introduction</a>
    <ul>
      <li><a href="#11-why-gradient-descent">1.1. Why gradient descent?</a></li>
      <li><a href="#12-methodology">1.2. Methodology</a></li>
    </ul>
  </li>
  <li><a href="#2-gradient-descent-for-linear-regression">2. Gradient Descent for Linear Regression</a>
    <ul>
      <li><a href="#21-matrix-derivatives">2.1. Matrix derivatives</a></li>
      <li><a href="#22-numerical-differentiation">2.2. Numerical Differentiation</a></li>
      <li><a href="#23-python-code-for-visualization">2.3. Python code for visualization</a></li>
    </ul>
  </li>
  <li><a href="#3-discussion">3. Discussion</a>
    <ul>
      <li><a href="#31-when-to-stop">3.1. When to stop?</a></li>
      <li><a href="#32-stucking-in-local-optimum">3.2. Stucking in Local Optimum</a></li>
      <li><a href="#33-speed-to-convergence-learning-rate">3.3. Speed to convergence (Learning rate)</a></li>
      <li><a href="#34-disadvantage-compared-to-using-formula">3.4. Disadvantage compared to using formula</a></li>
      <li><a href="#35-speedup-gd">3.5. Speedup GD</a></li>
    </ul>
  </li>
  <li><a href="#4-reference">4. Reference</a></li>
</ul>

<!-- /MarkdownTOC -->

<p><a name="1-introduction"></a></p>
<h2 id="1-introduction">1. Introduction</h2>
<p><a name="11-why-gradient-descent"></a></p>
<h3 id="11-why-gradient-descent">1.1. Why gradient descent?</h3>
<p>Given an optimization problem: Find \(x\) so that \(f(x)=x^4-5x^2-x+3\) as in <a href="#fig1">figure 1</a> reaches minumum value, an approach is to solve \(f'(x)=0\) to find all local minima then compare them to get global minimum. In our example, solving \(f'(x)=0\) gives us 2 local minima at \(x=x_1\) and \(x=x_2\), because \(f(x_1)&gt;f(x_2)\) hence \(f(x)\) reaches minimum at \(x=x_2\).</p>

<p>The problem with the approach above is that sometimes, <strong>the equation \(f'(x)=0\) cannot be solved easily</strong>. In this case, we can use an algorithm call <strong>gradient descent</strong> to find the <em>approximate</em> solution.
<a name="12-methodology"></a></p>
<h3 id="12-methodology">1.2. Methodology</h3>
<div class="message">Gradient descent (GD) is an iterative optimization problem algorithm for finding the minimum of a function. To find a local minimum of a function using GD, one takes steps proportional to the negative of the gradient (or of the approximate gradient) of the function at the current point.</div>

<p>To be more specific, GD will iteratively change the value of \(x\) \((x:=x+\beta)\) so that in each iteration, hopefully, \(f(x)\) is getting smaller and moving closer to the minimum.</p>

<p>One way to ensure that the adjustment \(\beta\) to \(x\) will cause \(f(x)\) smaller is to make \(\beta\) equal a <strong>portion of the negative of the gradient</strong>: \(\beta=-\alpha f'(x)\) where \(\alpha\) is a positive number and it’s called <strong>learning rate</strong>. In conclusion, the adjustment to \(x\) will be:</p>

<p>\begin{equation} \tag{1} \label{eq:1}
x := x - \alpha f’(x)
\end{equation}</p>

<p>Considering the point \(x_0\) in <a href="#fig1">figure 1</a>, because \(f'(x0)&lt;0\) so \(\alpha f'(x) &lt;0\) which means the update to \(x\) as in equation (\ref{eq:1})	 will move \(x\) to the right hand side, making \(f(x)\) descending.</p>
<div class="imgcap" id="fig1">
	<img style="display: inline-block; width: 60%" src="/public/post-assets/GradientDescent/fig1.png" width="500" align="center" />
	<div class="thecap">Figure 1</div>
</div>
<div style="clear:right;"></div>

<p><a name="2-gradient-descent-for-linear-regression"></a></p>
<h2 id="2-gradient-descent-for-linear-regression">2. Gradient Descent for Linear Regression</h2>
<p>Before we move on to the implementation and visualization, let’s quickly go through the concept of matrix derivative (to work with multi-dimensional data) and numerical differentiation (to calculate approximate gradient at a specific value of \(x\))
<a name="21-matrix-derivatives"></a></p>
<h3 id="21-matrix-derivatives">2.1. Matrix derivatives</h3>
<p>In our previous example, \(x\) is one-dimensional vector, but it’s not likely the case in most problem, \(x\) could a vector in n-dimensional space. In this case, we need to update all element in \(x\) <strong>simutaneously</strong>. If we don’t update them simutaneously then \(f'(x)\) will change everytime an element in \(x\) is updated. Matrix derivatives could help us archieve that.</p>

<p>The following equation is the formula for derivatives of \(f(A)\) with respect to \(m\)x\(n\) matrix \(A\).</p>

<p>\begin{equation}
\triangledown_A f(A) = \begin{bmatrix}
\frac{\partial f}{\partial A_{11}} &amp; \cdots  &amp; \frac{\partial f}{\partial A_{1n}} \newline
 \vdots &amp; \ddots &amp; \vdots \newline
 \frac{\partial f}{\partial A_{m1}} &amp; \cdots &amp; \frac{\partial f}{\partial A_{mn}}
\end{bmatrix} 
\end{equation}</p>

<p>In linear regression, \(x\) is a vector. Formula for derivatives with repsect to a vector:</p>

<p>\begin{equation}
\triangledown_x f(x) = \begin{bmatrix}
\frac{\partial f}{\partial x_{1}}\newline
 \vdots \newline
 \frac{\partial f}{\partial x_{m}} 
\end{bmatrix} 
\end{equation}</p>

<p><a name="22-numerical-differentiation"></a></p>
<h3 id="22-numerical-differentiation">2.2. Numerical Differentiation</h3>
<p>Numerical Differentiation can be used to check whether our gradient function (in code) is correct or not. When testing, give some value \(x\) then check whether or not \(f'(x)\) calculated by equation \eqref{eq:2} is closed enough to \(f'(x)\) calculated by equation \eqref{eq:3}.
\begin{equation} \tag{2} \label{eq:2}
f’(x) = \lim_{\varepsilon \rightarrow 0}\frac{f(x + \varepsilon) - f(x)}{\varepsilon}
\end{equation}</p>

<p>\begin{equation} \tag{3} \label{eq:3}
f’(x) \approx \frac{f(x + \varepsilon) - f(x - \varepsilon)}{2\varepsilon} 
\end{equation}</p>

<p>The proof of numerical differentiation can be found at <a href="https://en.wikipedia.org/wiki/Numerical_differentiation">Wikipedia</a>
<a name="23-python-code-for-visualization"></a></p>
<h3 id="23-python-code-for-visualization">2.3. Python code for visualization</h3>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">matplotlib</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="n">sklearn</span> <span class="kn">import</span> <span class="n">linear_model</span>
<span class="kn">import</span> <span class="n">matplotlib.animation</span> <span class="k">as</span> <span class="n">animation</span>
<span class="kn">import</span> <span class="n">os</span>
<span class="kn">import</span> <span class="n">subprocess</span>

<span class="c1"># apply gradient descent algorithm to optimiza cost function of several variables
</span>
<span class="k">def</span> <span class="nf">cost</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
	<span class="sh">"""</span><span class="s">Cost function of linear regression (LR) model with coefficient x</span><span class="sh">"""</span>
	<span class="n">m</span> <span class="o">=</span> <span class="n">A</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> 
	<span class="k">return</span> <span class="p">.</span><span class="mi">5</span><span class="o">/</span><span class="n">m</span> <span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">A</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">-</span><span class="n">b</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>

<span class="k">def</span> <span class="nf">grad</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
	<span class="sh">"""</span><span class="s">Gradient function of cost function</span><span class="sh">"""</span>
	<span class="n">m</span> <span class="o">=</span> <span class="n">A</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
	<span class="k">return</span> <span class="mi">1</span><span class="o">/</span><span class="n">m</span> <span class="o">*</span><span class="n">A</span><span class="p">.</span><span class="n">T</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">A</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">-</span><span class="n">b</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">numerical_grad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">cost</span><span class="p">):</span>
	<span class="sh">"""</span><span class="s">Calculating numerical gradient with coefficient x to check whether our grad function is correct or not</span><span class="sh">"""</span>
	<span class="n">eps</span> <span class="o">=</span> <span class="mf">1e-4</span>
	<span class="n">g</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
	<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">x</span><span class="p">)):</span>
		<span class="n">x_p</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">copy</span><span class="p">()</span>
		<span class="n">x_n</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">copy</span><span class="p">()</span>
		<span class="n">x_p</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="n">eps</span> 
		<span class="n">x_n</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-=</span> <span class="n">eps</span>
		<span class="n">g</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="nf">cost</span><span class="p">(</span><span class="n">x_p</span><span class="p">)</span> <span class="o">-</span> <span class="nf">cost</span><span class="p">(</span><span class="n">x_n</span><span class="p">))</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">eps</span><span class="p">)</span>
	<span class="k">return</span> <span class="n">g</span> 

<span class="k">def</span> <span class="nf">check_grad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">cost</span><span class="p">,</span> <span class="n">grad</span><span class="p">):</span>
	<span class="sh">"""</span><span class="s">Check grad function by comparing actual gradient value and numerical gradient (estimation)</span><span class="sh">"""</span>
	<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
	<span class="n">grad1</span> <span class="o">=</span> <span class="nf">grad</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
	<span class="n">grad2</span> <span class="o">=</span> <span class="nf">numerical_grad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">cost</span><span class="p">)</span>
	<span class="k">return</span> <span class="bp">True</span> <span class="k">if</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">grad1</span> <span class="o">-</span> <span class="n">grad2</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mf">1e-6</span> <span class="k">else</span> <span class="bp">False</span> 

<span class="k">def</span> <span class="nf">gradient_descent</span><span class="p">(</span><span class="n">x_init</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">alpha</span><span class="p">):</span>
	<span class="sh">"""</span><span class="s">
	Running gradient descent with initial value x_init, grad function, learning rate alpha 
	This function return x: list of solution after each iteration and iteration: last iteration it has gone through
	</span><span class="sh">"""</span>
	<span class="n">m</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">x_init</span><span class="p">)</span>
	<span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="n">x_init</span><span class="p">]</span>
	<span class="k">for</span> <span class="n">iteration</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">itr</span><span class="p">):</span>
		<span class="n">x_new</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">alpha</span><span class="o">*</span><span class="nf">grad</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span> 
		<span class="k">if</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="nf">grad</span><span class="p">(</span><span class="n">x_new</span><span class="p">))</span><span class="o">/</span><span class="n">m</span> <span class="o">&lt;</span> <span class="mf">1e-3</span><span class="p">:</span> <span class="c1">#Stop gradient descent when grad value is too small.
</span>			<span class="k">break</span>
		<span class="n">x</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">x_new</span><span class="p">)</span>
	<span class="nf">return </span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">iteration</span><span class="p">)</span>

<span class="c1">############### Main ####################
</span>
<span class="c1"># create random data 
</span><span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mi">2</span><span class="p">,</span><span class="mi">9</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="mi">9</span><span class="p">,</span><span class="mi">11</span><span class="p">,</span><span class="mi">16</span><span class="p">,</span><span class="mi">25</span><span class="p">,</span><span class="mi">23</span><span class="p">,</span><span class="mi">22</span><span class="p">,</span><span class="mi">29</span><span class="p">,</span><span class="mi">29</span><span class="p">,</span><span class="mi">35</span><span class="p">,</span><span class="mi">37</span><span class="p">,</span><span class="mi">40</span><span class="p">,</span><span class="mi">46</span><span class="p">]]).</span><span class="n">T</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="mi">8</span><span class="p">,</span><span class="mi">9</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">11</span><span class="p">,</span><span class="mi">12</span><span class="p">,</span><span class="mi">13</span><span class="p">,</span><span class="mi">14</span><span class="p">,</span><span class="mi">15</span><span class="p">,</span><span class="mi">16</span><span class="p">]]).</span><span class="n">T</span>

<span class="n">fig1</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="sh">'</span><span class="s">GD for Linear Regression</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">axes</span><span class="p">(</span><span class="n">xlim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">60</span><span class="p">),</span> <span class="n">ylim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">20</span><span class="p">))</span> <span class="c1">#restrict the figure showing only values in specified range
</span><span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="n">b</span><span class="p">,</span><span class="sh">'</span><span class="s">ro</span><span class="sh">'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">_nolegend_</span><span class="sh">'</span><span class="p">)</span> <span class="c1"># plot data points
</span><span class="n">line</span><span class="p">,</span> <span class="o">=</span> <span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">([],</span> <span class="p">[],</span> <span class="n">color</span> <span class="o">=</span> <span class="sh">'</span><span class="s">blue</span><span class="sh">'</span><span class="p">)</span> <span class="c1"># plot function used in animation
</span>
<span class="c1"># plot solution found by scikit learn
</span><span class="n">lr</span> <span class="o">=</span> <span class="n">linear_model</span><span class="p">.</span><span class="nc">LinearRegression</span><span class="p">()</span>
<span class="n">lr</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="n">b</span><span class="p">)</span>
<span class="n">x0_gd</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">46</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="n">y0_sklearn</span> <span class="o">=</span> <span class="n">lr</span><span class="p">.</span><span class="n">intercept_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">lr</span><span class="p">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">x0_gd</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x0_gd</span><span class="p">,</span> <span class="n">y0_sklearn</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">green</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># apply gradient descent
</span><span class="n">itr</span> <span class="o">=</span> <span class="mi">90</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="p">.</span><span class="mi">0001</span> <span class="c1">#.0001
</span><span class="n">x_init</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">]])</span>

<span class="n">ones</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="n">A</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">int8</span><span class="p">)</span> <span class="c1">#Append bias to A
</span><span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">concatenate</span><span class="p">((</span><span class="n">ones</span><span class="p">,</span><span class="n">A</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Checking gradient...</span><span class="sh">'</span><span class="p">,</span> <span class="nf">check_grad</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">cost</span><span class="p">,</span> <span class="n">grad</span><span class="p">))</span> <span class="c1">#Output: Checking gradient... True
</span>
<span class="n">x_init</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">]])</span>

<span class="c1"># running gradient descent
</span><span class="n">myGD</span> <span class="o">=</span> <span class="nf">gradient_descent</span><span class="p">(</span><span class="n">x_init</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">)</span>

<span class="c1"># plot x_init (black line)
</span><span class="n">x0_gd</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">46</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="n">y0_init</span> <span class="o">=</span> <span class="n">x_init</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">x_init</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">x0_gd</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x0_gd</span><span class="p">,</span> <span class="n">y0_init</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="sh">'</span><span class="s">black</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># plot lines in each iteration
</span><span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="n">i</span><span class="p">):</span>
	<span class="n">x_gd</span> <span class="o">=</span> <span class="n">myGD</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span><span class="p">]</span>
	<span class="n">y0_gd</span> <span class="o">=</span> <span class="n">x_gd</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">x_gd</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">x0_gd</span>
	<span class="n">line</span><span class="p">.</span><span class="nf">set_data</span><span class="p">(</span><span class="n">x0_gd</span><span class="p">,</span><span class="n">y0_gd</span><span class="p">)</span>
	<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Iteration: {}/{}, learning rate: {}</span><span class="sh">'</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">myGD</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">learning_rate</span><span class="p">))</span>
	<span class="k">return</span> <span class="n">line</span><span class="p">,</span>

<span class="c1"># legend for graph
</span><span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">((</span><span class="sh">'</span><span class="s">Value in each GD iteration</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Solution by formular</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Inital value for GD</span><span class="sh">'</span><span class="p">),</span> <span class="n">loc</span><span class="o">=</span><span class="p">(</span><span class="mf">0.52</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">))</span>
<span class="n">ltext</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">gca</span><span class="p">().</span><span class="nf">get_legend</span><span class="p">().</span><span class="nf">get_texts</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">setp</span><span class="p">(</span><span class="n">ltext</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">setp</span><span class="p">(</span><span class="n">ltext</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">setp</span><span class="p">(</span><span class="n">ltext</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="c1"># save animation to mp4 file using ffmpeg
</span><span class="n">Writer</span> <span class="o">=</span> <span class="n">animation</span><span class="p">.</span><span class="n">writers</span><span class="p">[</span><span class="sh">'</span><span class="s">ffmpeg</span><span class="sh">'</span><span class="p">]</span>
<span class="n">writer</span> <span class="o">=</span> <span class="nc">Writer</span><span class="p">(</span><span class="n">fps</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">metadata</span><span class="o">=</span><span class="nf">dict</span><span class="p">(</span><span class="n">artist</span><span class="o">=</span><span class="sh">'</span><span class="s">Me</span><span class="sh">'</span><span class="p">),</span> <span class="n">bitrate</span><span class="o">=</span><span class="mi">1800</span><span class="p">)</span>

<span class="n">line_ani</span> <span class="o">=</span> <span class="n">animation</span><span class="p">.</span><span class="nc">FuncAnimation</span><span class="p">(</span><span class="n">fig1</span><span class="p">,</span> <span class="n">update</span><span class="p">,</span> <span class="n">myGD</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">interval</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">blit</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="c1"># # Save mp4 file for animation, require ffmpeg and set path to environment variable.
# line_ani.save('lines.mp4', writer='ffmpeg')
</span>
<span class="c1"># # Convert mp4 to gif
# # https://stackoverflow.com/questions/11269575/how-to-hide-output-of-subprocess-in-python-2-7
# FNULL = open(os.devnull, 'w')
# subprocess.call(['ffmpeg', '-i', 'lines.mp4', 'lines.gif'], stdout=FNULL, stderr=subprocess.STDOUT) # Execute command line to convert .mp4 to .gif using ffmpeg and hide output of command line to terminal
</span><span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="sh">'</span><span class="s">Iter and cost function</span><span class="sh">'</span><span class="p">)</span>

<span class="n">cost_list</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">iter_list</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">itr</span><span class="p">):</span>
	<span class="n">iter_list</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">itr</span><span class="p">):</span>
	<span class="n">cost_list</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="nf">cost</span><span class="p">(</span><span class="n">myGD</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span><span class="p">]))</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">iter_list</span><span class="p">,</span> <span class="n">cost_list</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Iteration</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Cost value</span><span class="sh">'</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span> 
</code></pre></div></div>
<div class="imgcap">
	<img style="display: inline-block; width: 60%;" src="/public/post-assets/GradientDescent/lines.gif" width="500" align="center" />
<div class="thecap">Code output</div>
</div>

<p><a name="3-discussion"></a></p>
<h2 id="3-discussion">3. Discussion</h2>
<p><a name="31-when-to-stop"></a></p>
<h3 id="31-when-to-stop">3.1. When to stop?</h3>
<p>The relationship between cost value in each iteration is shown in <a href="#2a">figure 2.a</a>, clearly, the cost value tends to bottoms out and remains stable after iteration 40. The rest iteration doesn’t seem to reduces the cost value as much and could be unnecessary. When the calculation is expensive, we might not want to continued the iteration while the solution is already good enough. So plotting cost value after each iteration or even calculating the slope of that function could be a way to decide when to stop GD.</p>

<p><a href="#2a">Figure 2.b</a> shows that setting \(1e-3\) as the threshold is not good enough to stop GD because in that example, the optimal solution occurs when \(\mid {grad} \mid \approx 5.94\).</p>
<div class="imgcap">
	<img id="2a" style="display: inline-block; float:left; width: 50%;" src="/public/post-assets/GradientDescent/cost_iter.png" width="500" align="center" />
	<div class="imgcap">
	<img style="display: inline-block; float:left; width: 50%;" src="/public/post-assets/GradientDescent/10000parabola.gif" width="500" align="center" />
	<div class="thecap">Figure 2.a: Relationship between cost value and iteration number</div>
<div class="thecap">Figure 2.b: GD stuck in local optimum</div>
</div>
<div class="thecap"></div>
</div>
<div style="clear:left"></div>

<p><a name="32-stucking-in-local-optimum"></a></p>
<h3 id="32-stucking-in-local-optimum">3.2. Stucking in Local Optimum</h3>

<p>In figure <a href="#fig3">(3)</a>, when trying to fit data by a parabola, GD get stuck in a local optimum, we know that because the green line is the solution found by formula (which is global optimum as I have explain in <a href="https://dunglai.github.io/2017/10/10/linear-regression/">this blog</a>). When trying to fit a parabola, another vector is added to the collumn space. When working with high dimensional space, it’s likely to have multiple local mimima. It means GD is very sensitive to initial value and it’s hard to get out of a local minimum. There are serveral variant of GD that can deal with this problem such as Stochastic Gradient Descent.</p>

<div class="imgcap">
	<img id="fig3" style="display: inline-block; width: 60%;" src="/public/post-assets/GradientDescent/100parabola.gif" width="500" align="center" />
	<div class="thecap">Figure 3: GD stucked in a local optimum</div>
</div>
<p><a href="dunglai.github.io/public/post-assets/GradientDescent/gd_parabola_100iter.py">Source Code for figure 3</a></p>

<p><a name="33-speed-to-convergence-learning-rate"></a></p>
<h3 id="33-speed-to-convergence-learning-rate">3.3. Speed to convergence (Learning rate)</h3>
<p>Learning rate (\(\alpha\)) is an important parameter, small learning rate as in figure 4.a can slow down GD and maybe makes it very slow to converge. On the other hand, large learning rate as in figure 4.b can make GD impossible to converge.</p>
<div class="imgcap">
	<img style="display: inline-block; width: 60%;" src="/public/post-assets/GradientDescent/learning_rate.jpg" width="500" align="center" />
	<div class="thecap">Figure 4.a: Small learning rates.</div>
	<div class="thecap">Figure 4.b: Large learning rates.</div>
</div>

<p><a name="34-disadvantage-compared-to-using-formula"></a></p>
<h3 id="34-disadvantage-compared-to-using-formula">3.4. Disadvantage compared to using formula</h3>

<table>
  <thead>
    <tr>
      <th>Gradient Descent</th>
      <th>Normal Equation</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Need to chose learning rate</td>
      <td>No need to choose learning rate</td>
    </tr>
    <tr>
      <td>Needs many iteration</td>
      <td>Don’t need to iterate</td>
    </tr>
    <tr>
      <td>Work well even when n (features) is large</td>
      <td>Need to compute projection matrix 0(n3)</td>
    </tr>
    <tr>
      <td>O(kn2)</td>
      <td>Slow if n (features) is very large</td>
    </tr>
  </tbody>
</table>

<p><a name="35-speedup-gd"></a></p>
<h3 id="35-speedup-gd">3.5. Speedup GD</h3>
<p><strong>Feature Scaling</strong> is commonly be used when working with GD, we want features are in similar scale (range) and it can be archieved by:</p>

<ul>
  <li><strong>Rescaling</strong>: The simplest method is rescaling the range of features to scale the range in [0, 1] or [−1, 1]. Selecting the target range depends on the nature of the data.</li>
</ul>

<p>\begin{equation}
x’=\frac{x-min(x)}{max(x)-min(x)}
\end{equation}</p>

<p>\(x\) is original value and \(x'\) is the normalized value.</p>

<ul>
  <li><strong>Mean normalisation</strong></li>
</ul>

<p>\begin{equation}
x’=\frac{x-mean(x)}{s_i}
\end{equation}</p>

<p>\(s_i\) can be standard deviation or \(s_i\) is the range of value (max-min).
<a name="4-reference"></a></p>
<h2 id="4-reference">4. Reference</h2>

<ol>
  <li><a href="https://machinelearningcoban.com/2017/01/12/gradientdescent/">Blog by Tiepvu</a></li>
  <li><a href="http://sebastianruder.com/optimizing-gradient-descent/">Blog by sebastianruder</a></li>
  <li><a href="http://cs229.stanford.edu/notes/cs229-notes1.pdf">Lecture note from Andrew Ng in machine learning course CS229</a></li>
  <li><a href="https://en.wikipedia.org/wiki/Feature_scaling">Feature Scaling</a></li>
  <li><a href="https://www.coursera.org/learn/machine-learning">Machine Learning course by Andrew Ng in Coursera</a></li>
</ol>

</div>

<div class="related">
  <h2>Related Posts</h2>
  <ul class="related-posts">
    
      <li>
        <h3>
          <a href="/2018/07/26/linked-list/">
            Linked List, Stack, Queue
            <small>26 Jul 2018</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/2018/07/10/cars-visualisation/">
            Interactive Visualisation
            <small>10 Jul 2018</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/2018/04/09/NavigationTrainer/">
            Journey Preparation Tool Project
            <small>09 Apr 2018</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/2018/01/25/SVD/">
            Singular Value Decomposition
            <small>25 Jan 2018</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/2017/12/09/k-nearest-neighbors/">
            K Nearest Neighbors
            <small>09 Dec 2017</small>
          </a>
        </h3>
      </li>
    
  </ul>
</div>
<div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://dunglai-github-io.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>

      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    <script>
      (function(document) {
        var toggle = document.querySelector('.sidebar-toggle');
        var sidebar = document.querySelector('#sidebar');
        var checkbox = document.querySelector('#sidebar-checkbox');

        document.addEventListener('click', function(e) {
          var target = e.target;

          if (!checkbox.checked ||
             sidebar.contains(target) ||
             (target === checkbox || target === toggle)) return;

          checkbox.checked = false;
        }, false);
      })(document);
    </script>
  </body>

  
</html>
